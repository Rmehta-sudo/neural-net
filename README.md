# ğŸš€ Go Micrograd: A Tiny Autograd Engine and Neural Network Library

A minimalist implementation of a scalar-valued automatic differentiation engine and small neural network library in Go, inspired by Andrej Karpathyâ€™s [micrograd](https://github.com/karpathy/micrograd). Designed to be **simple, educational, and fully self-contained**, showing the core mechanics of backpropagation and neural network training from scratch.

---

## âœ¨ Features
- **Scalar Autograd Engine** â€” Tracks data, gradients, and builds a dynamic computation graph.
- **Basic Operations & Activations** â€” `+`, `-`, `*`, `/`, `^`, `tanh` with automatic gradient calculation.
- **Neural Network Components**:
  - **Neuron** â€” Single perceptron with weights, bias, and `tanh` activation.
  - **Layer** â€” A collection of neurons.
  - **MLP** â€” Multi-Layer Perceptron with multiple layers.
- **Gradient Descent Training** â€” Simple loop for updating parameters.
- **Readable Code** â€” Easy to follow and modify.

---

## ğŸ Getting Started

### Prerequisites
- Go **1.18+**

### Installation
```bash
git clone https://github.com/Rmehta-sudo/neural-net.git
cd neural-net
go run main.go
```
`main.go` contains demonstrations of:
- Scalar autograd (`TestValue`)
- Single neuron (`TestNeuron`)
- Layer (`TestLayer`)
- MLP (`TestMLP` â€” full training example)

---

## ğŸ“‚ Project Structure
```
neural-net/
â”œâ”€â”€ main.go               # Entry point with usage examples
â””â”€â”€ engine/
    â”œâ”€â”€ value.go          # Core Value type (data, gradient, autograd logic)
    â”œâ”€â”€ neuron.go         # Neuron implementation
    â”œâ”€â”€ layer.go          # Layer of neurons
    â””â”€â”€ mlp.go            # Multi-Layer Perceptron
```

---

## ğŸ”¹ `engine/value.go` (Core Autograd)
`Value` represents a scalar in the computation graph with:
- **Data** â€” The numeric value.
- **Grad** â€” Gradient w.r.t. final loss.
- **Prev** â€” Previous `Value` objects (graph links).
- **Op** â€” Operation type (e.g., `+`, `*`, `tanh`).

Key methods:
- `NewValue(val, label)` â€” Create a new value.
- `Add`, `Sub`, `Mul`, `Div`, `Pow` â€” Arithmetic ops.
- `Tanh()` â€” Activation.
- `FullBackward()` â€” Backprop through the graph.

---

## ğŸ›  Usage
Example from `TestMLP` â€” binary classification:
```go
xs := [][]float64{
    {2.0, 3.0, -1.0},
    {3.0, -1.0, 0.5},
    {0.5, 1.0, 1.0},
    {1.0, 1.0, -1.0},
}
ys := []float64{1.0, -1.0, -1.0, 1.0}
mlp := engine.NewMLP([]int{4, 4, 1}, 3) // 3 inputs â†’ 4 â†’ 4 â†’ 1
```
Training loop:
1. **Forward Pass** â€” `mlp.Output(inputs)`
2. **Loss Calculation** â€” Mean Squared Error (MSE)
3. **Backward Pass** â€” `loss.FullBackward()`
4. **Gradient Descent** â€” Update parameters
5. **Reset Gradients** â€” Set `p.Grad = 0`

---

## ğŸ“‹ Custom Training Example (XOR)
```go
package main

import (
    "fmt"
    "github.com/Rmehta-sudo/neural-net/engine"
    "math/rand"
    "time"
)

func main() {
    rand.Seed(time.Now().UnixNano())

    inputs := [][]float64{
        {0, 0}, {0, 1}, {1, 0}, {1, 1},
    }
    targets := []float64{0, 1, 1, 0}

    mlp := engine.NewMLP([]int{4, 1}, 2)
    xVals := engine.ToValue2D(inputs)
    yVals := engine.ToValue1D(targets)

    lr := 0.03
    iters := 10000
    params := mlp.Parameters()

    for i := 0; i < iters; i++ {
        var preds []*engine.Value
        for _, x := range xVals {
            preds = append(preds, mlp.Output(x)[0])
        }

        loss := engine.NewValue(0.0, "loss")
        for j, p := range preds {
            diff := p.Sub(yVals[j])
            loss = loss.Add(diff.Mul(diff))
        }

        loss.FullBackward()

        for _, p := range params {
            p.Data -= lr * p.Grad
            p.Grad = 0
        }

        if i%(iters/10) == 0 {
            fmt.Printf("Iter %d, Loss: %.6f\n", i, loss.Data)
        }
    }

    for i, x := range xVals {
        fmt.Printf("Input: %v, Pred: %.4f\n", inputs[i], mlp.Output(x)[0].Data)
    }
}
```

---

## ğŸ“œ License
MIT License â€” see [LICENSE](LICENSE) file.

---

## ğŸ™ Acknowledgements
- Inspired by [Andrej Karpathy's micrograd](https://github.com/karpathy/micrograd)
- His YouTube lectures on neural networks and backpropagation
